{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Initialize the Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Initialize Glue Context\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 79b238aa-1903-4066-9b56-a979eaf981cd\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 79b238aa-1903-4066-9b56-a979eaf981cd to get into ready status...\nSession 79b238aa-1903-4066-9b56-a979eaf981cd has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col, from_unixtime, to_timestamp\nimport pyspark.sql.functions as f",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read and Filter the Reviews Dataset",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Replace with your S3 paths\nreviews_input = \"s3://recommendation-project-rapid/raw/All_Beauty.jsonl\"\n\n# Load Reviews\ndf_reviews = spark.read.json(reviews_input)\n\n# Select only the columns needed for the recommendation engine\n# We use 'parent_asin' as the join key\nreviews_cleaned = (\n    df_reviews.select(\n    col(\"user_id\"),\n    col(\"parent_asin\"),\n    col(\"rating\").cast(\"double\"),\n    col(\"timestamp\") #.cast(\"long\").alias(\"event_time\")\n    )\n    .withColumn(\n        \"event_time_seconds\", (col(\"timestamp\") / 1000).cast(\"double\")\n    ).withColumn(\n        \"calendar_date\", from_unixtime(col(\"event_time_seconds\"), \"yyyy-MM-dd\")\n    )\n    .drop(\"timestamp\")\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "reviews_cleaned.select(f.min(\"calendar_date\"), f.max(\"calendar_date\")).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+------------------+\n|min(calendar_date)|max(calendar_date)|\n+------------------+------------------+\n|        2000-11-01|        2023-09-09|\n+------------------+------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_reviews.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "701528\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read and Filter the Metadata Dataset",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "metadata_input = \"s3://recommendation-project-rapid/raw/meta_All_Beauty.jsonl\"\n\n# Load Metadata\ndf_metadata = spark.read.json(metadata_input)\n\n# Select only the columns needed\nmetadata_cleaned = df_metadata.select(\n    col(\"parent_asin\").alias(\"meta_parent_asin\"), # Avoid name collision during join\n    col(\"title\").alias(\"movie_title\"),\n    col(\"main_category\")\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Join the Datasets",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Join on parent_asin\nfinal_df = reviews_cleaned.join(\n    metadata_cleaned, \n    reviews_cleaned.parent_asin == metadata_cleaned.meta_parent_asin, \n    \"inner\"\n).drop(\"meta_parent_asin\") # Drop the duplicate key\n\n# Preview the joined result\nfinal_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-----------+------+------------------+-------------+--------------------+-------------+\n|             user_id|parent_asin|rating|event_time_seconds|calendar_date|         movie_title|main_category|\n+--------------------+-----------+------+------------------+-------------+--------------------+-------------+\n|AGKHLEW2SOWHNMFQI...| B00YQ6X8EO|   5.0|  1.588687728923E9|   2020-05-05|Herbivore - Natur...|   All Beauty|\n|AGKHLEW2SOWHNMFQI...| B081TJ8YS3|   4.0|   1.58861585507E9|   2020-05-04|All Natural Vegan...|   All Beauty|\n|AE74DYR3QUGVPZJ3P...| B097R46CSY|   5.0|  1.589665266052E9|   2020-05-16|New Road Beauty -...|   All Beauty|\n|AFQLNQNQYFWQZPJQZ...| B09JS339BZ|   1.0|   1.64339363022E9|   2022-01-28|muaowig Ombre Bod...|   All Beauty|\n|AFQLNQNQYFWQZPJQZ...| B08BZ63GMJ|   5.0|  1.609322563534E9|   2020-12-30|Yinhua Electric N...|   All Beauty|\n+--------------------+-----------+------+------------------+-------------+--------------------+-------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "final_df.select(f.min(\"calendar_date\"), f.max(\"calendar_date\")).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+------------------+\n|min(calendar_date)|max(calendar_date)|\n+------------------+------------------+\n|        2000-11-01|        2023-09-09|\n+------------------+------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Save as Optimized Parquet",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "output_path = \"s3://recommendation-project-rapid/processed/all_beauty_dataset/\"\n\n# Write to S3 in Parquet format with Snappy compression\nfinal_df.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"rating\") \\\n    .parquet(output_path)\n\nprint(f\"Job Complete! Joined data saved to: {output_path}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "Job Complete! Joined data saved to: s3://recommendation-project-rapid/processed/all_beauty_dataset/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}