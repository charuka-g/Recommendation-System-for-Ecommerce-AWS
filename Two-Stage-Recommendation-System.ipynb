{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Two-Stage Recommendation System\n",
        "## Retrieval (Matrix Factorization) + Ranking (XGBoost)\n",
        "\n",
        "This notebook implements a two-stage recommendation system:\n",
        "1. **Retrieval Stage**: Factorization Machines to generate embeddings and retrieve top-100 candidates\n",
        "2. **Ranking Stage**: XGBoost to rank candidates using rich metadata from Feature Store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import io\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple\n",
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "from sagemaker.feature_store.feature_store import FeatureStore\n",
        "from sagemaker import image_uris\n",
        "import sagemaker.amazon.common as smac\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pickle\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "region = \"ap-south-1\"\n",
        "role_arn = \"arn:aws:iam::487512486150:role/recommendationsystem-sagemaker-role\"\n",
        "bucket = \"amazon-sagemaker-local-dev-store\"\n",
        "\n",
        "# Feature Group Names (Update these with your actual Feature Group names)\n",
        "USER_FEATURE_GROUP_NAME = \"all-beauty-users-<timestamp>\"  # Replace with actual name\n",
        "ITEM_FEATURE_GROUP_NAME = \"all-beauty-items-<timestamp>\"  # Replace with actual name\n",
        "\n",
        "# Initialize sessions\n",
        "boto_session = boto3.Session(region_name=region)\n",
        "sagemaker_session = sagemaker.Session(\n",
        "    boto_session=boto_session,\n",
        "    default_bucket=bucket\n",
        ")\n",
        "\n",
        "# Feature Store clients\n",
        "featurestore_runtime = boto_session.client(\n",
        "    service_name='sagemaker-featurestore-runtime',\n",
        "    region_name=region\n",
        ")\n",
        "sagemaker_client = boto_session.client(\n",
        "    service_name='sagemaker',\n",
        "    region_name=region\n",
        ")\n",
        "\n",
        "feature_store = FeatureStore(sagemaker_session=sagemaker_session)\n",
        "\n",
        "print(f\"Initialized SageMaker session in {region}\")\n",
        "print(f\"Default bucket: {bucket}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Retrieval Stage: Matrix Factorization with Factorization Machines\n",
        "\n",
        "### 2.1 Prepare Training Data for Factorization Machines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_fm_training_data(\n",
        "    interactions_df: pd.DataFrame,\n",
        "    output_s3_path: str\n",
        ") -> Tuple[str, Dict, int]:\n",
        "    \"\"\"\n",
        "    Prepare training data for Factorization Machines.\n",
        "    \n",
        "    Factorization Machines requires a specific format:\n",
        "    - Each line: label |user_id |item_id\n",
        "    - user_id and item_id are categorical features (one-hot encoded)\n",
        "    \n",
        "    Args:\n",
        "        interactions_df: DataFrame with columns [user_id, parent_asin, rating]\n",
        "        output_s3_path: S3 path to save the training data\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (s3_uri, mappings_dict, feature_dim)\n",
        "    \"\"\"\n",
        "    # Create mappings for user_id and parent_asin to indices\n",
        "    unique_users = interactions_df['user_id'].unique()\n",
        "    unique_items = interactions_df['parent_asin'].unique()\n",
        "    \n",
        "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
        "    \n",
        "    # Calculate feature dimensions\n",
        "    num_users = len(unique_users)\n",
        "    num_items = len(unique_items)\n",
        "    feature_dim = num_users + num_items  # Total one-hot dimensions\n",
        "    \n",
        "    print(f\"Number of unique users: {num_users}\")\n",
        "    print(f\"Number of unique items: {num_items}\")\n",
        "    print(f\"Feature dimension: {feature_dim}\")\n",
        "    \n",
        "    # Convert to sparse format for FM\n",
        "    # Format: label |user_feature_index |item_feature_index\n",
        "    records = []\n",
        "    for _, row in interactions_df.iterrows():\n",
        "        user_idx = user_to_idx[row['user_id']]\n",
        "        item_idx = item_to_idx[row['parent_asin']] + num_users  # Offset by num_users\n",
        "        rating = row['rating']\n",
        "        \n",
        "        # FM format: label |feature_index:value\n",
        "        record = f\"{rating} |{user_idx}:1.0 |{item_idx}:1.0\"\n",
        "        records.append(record)\n",
        "    \n",
        "    # Save to local file first\n",
        "    local_file = '/tmp/fm_training_data.txt'\n",
        "    with open(local_file, 'w') as f:\n",
        "        f.write('\\n'.join(records))\n",
        "    \n",
        "    # Upload to S3\n",
        "    s3_client = boto_session.client('s3')\n",
        "    s3_path = output_s3_path.replace(f's3://{bucket}/', '')\n",
        "    s3_client.upload_file(local_file, bucket, s3_path)\n",
        "    \n",
        "    s3_uri = f\"s3://{bucket}/{s3_path}\"\n",
        "    print(f\"Training data saved to: {s3_uri}\")\n",
        "    \n",
        "    # Save mappings for later use\n",
        "    mappings = {\n",
        "        'user_to_idx': user_to_idx,\n",
        "        'item_to_idx': item_to_idx,\n",
        "        'idx_to_user': {idx: user for user, idx in user_to_idx.items()},\n",
        "        'idx_to_item': {idx: item for item, idx in item_to_idx.items()},\n",
        "        'num_users': num_users,\n",
        "        'num_items': num_items,\n",
        "        'feature_dim': feature_dim\n",
        "    }\n",
        "    \n",
        "    # Save mappings to S3\n",
        "    mappings_key = s3_path.replace('training_data.txt', 'mappings.pkl')\n",
        "    mappings_buffer = io.BytesIO()\n",
        "    pickle.dump(mappings, mappings_buffer)\n",
        "    mappings_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(mappings_buffer, bucket, mappings_key)\n",
        "    \n",
        "    return s3_uri, mappings, feature_dim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Train Factorization Machines Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_factorization_machines(\n",
        "    training_data_s3_uri: str,\n",
        "    feature_dim: int,\n",
        "    num_factors: int = 64,\n",
        "    epochs: int = 10,\n",
        "    instance_type: str = 'ml.c5.xlarge'\n",
        ") -> sagemaker.estimator.Estimator:\n",
        "    \"\"\"\n",
        "    Train a Factorization Machines model for collaborative filtering.\n",
        "    \n",
        "    Args:\n",
        "        training_data_s3_uri: S3 URI of training data\n",
        "        feature_dim: Dimension of feature space (num_users + num_items)\n",
        "        num_factors: Number of latent factors\n",
        "        epochs: Number of training epochs\n",
        "        instance_type: SageMaker instance type for training\n",
        "    \n",
        "    Returns:\n",
        "        Trained SageMaker estimator\n",
        "    \"\"\"\n",
        "    # Get Factorization Machines container\n",
        "    container = image_uris.retrieve(\n",
        "        \"factorization-machines\",\n",
        "        boto_session.region_name,\n",
        "        version='1'\n",
        "    )\n",
        "    \n",
        "    # Create estimator\n",
        "    fm_estimator = sagemaker.estimator.Estimator(\n",
        "        container,\n",
        "        role=role_arn,\n",
        "        instance_count=1,\n",
        "        instance_type=instance_type,\n",
        "        output_path=f\"s3://{bucket}/fm-model-artifacts/\",\n",
        "        sagemaker_session=sagemaker_session\n",
        "    )\n",
        "    \n",
        "    # Set hyperparameters\n",
        "    fm_estimator.set_hyperparameters(\n",
        "        feature_dim=feature_dim,\n",
        "        predictor_type='regressor',  # Predicting rating (continuous value)\n",
        "        num_factors=num_factors,     # Latent factors for embeddings\n",
        "        epochs=epochs\n",
        "    )\n",
        "    \n",
        "    # Start training\n",
        "    print(\"Starting Factorization Machines training...\")\n",
        "    fm_estimator.fit({'train': training_data_s3_uri})\n",
        "    \n",
        "    print(f\"Training complete! Model artifacts: {fm_estimator.model_data}\")\n",
        "    \n",
        "    return fm_estimator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Extract User and Item Embeddings\n",
        "\n",
        "**Note**: Factorization Machines models store embeddings in the weight matrix. We need to download the model artifacts and extract the V (factorization) matrix which contains the embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_embeddings(\n",
        "    fm_estimator: sagemaker.estimator.Estimator,\n",
        "    mappings: Dict,\n",
        "    num_factors: int = 64\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Extract user and item embeddings from trained FM model.\n",
        "    \n",
        "    The FM model learns latent factors (embeddings) for each user and item.\n",
        "    We need to download the model artifacts and extract the weight matrices.\n",
        "    \n",
        "    Args:\n",
        "        fm_estimator: Trained FM estimator\n",
        "        mappings: Dictionary with user/item mappings\n",
        "        num_factors: Number of latent factors\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (user_embeddings, item_embeddings, embedding_dict)\n",
        "    \"\"\"\n",
        "    import tarfile\n",
        "    try:\n",
        "        import mxnet as mx\n",
        "    except ImportError:\n",
        "        print(\"Installing mxnet...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", \"mxnet\"])\n",
        "        import mxnet as mx\n",
        "    \n",
        "    # Download model artifacts\n",
        "    model_uri = fm_estimator.model_data\n",
        "    local_model_path = '/tmp/fm_model.tar.gz'\n",
        "    \n",
        "    s3_client = boto_session.client('s3')\n",
        "    # Parse S3 URI\n",
        "    s3_path = model_uri.replace('s3://', '')\n",
        "    bucket_name, key = s3_path.split('/', 1)\n",
        "    \n",
        "    print(f\"Downloading model from {model_uri}...\")\n",
        "    s3_client.download_file(bucket_name, key, local_model_path)\n",
        "    \n",
        "    # Extract tar.gz\n",
        "    with tarfile.open(local_model_path, 'r:gz') as tar:\n",
        "        tar.extractall('/tmp/fm_model')\n",
        "    \n",
        "    # Load MXNet model (FM uses MXNet)\n",
        "    model_path = '/tmp/fm_model/model'\n",
        "    sym, arg_params, aux_params = mx.model.load_checkpoint(model_path, 0)\n",
        "    \n",
        "    # Extract weight matrices\n",
        "    # FM model structure: linear weights (w) and factorized weights (V)\n",
        "    # V matrix contains the embeddings: shape (feature_dim, num_factors)\n",
        "    \n",
        "    # Find the V (embedding) matrix in the parameters\n",
        "    v_key = None\n",
        "    for key in arg_params.keys():\n",
        "        if 'v' in key.lower() or 'factor' in key.lower():\n",
        "            v_key = key\n",
        "            break\n",
        "    \n",
        "    if v_key is None:\n",
        "        # Fallback: assume standard FM parameter naming\n",
        "        v_key = 'v'\n",
        "    \n",
        "    # Get embedding matrix\n",
        "    V = arg_params[v_key].asnumpy()  # Shape: (feature_dim, num_factors)\n",
        "    \n",
        "    num_users = mappings['num_users']\n",
        "    num_items = mappings['num_items']\n",
        "    \n",
        "    # Split V into user and item embeddings\n",
        "    user_embeddings = V[:num_users, :]  # Shape: (num_users, num_factors)\n",
        "    item_embeddings = V[num_users:num_users+num_items, :]  # Shape: (num_items, num_factors)\n",
        "    \n",
        "    print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
        "    print(f\"Item embeddings shape: {item_embeddings.shape}\")\n",
        "    \n",
        "    # Create lookup dictionaries\n",
        "    embedding_dict = {}\n",
        "    \n",
        "    # User embeddings\n",
        "    for user_id, idx in mappings['user_to_idx'].items():\n",
        "        embedding_dict[f\"user_{user_id}\"] = user_embeddings[idx]\n",
        "    \n",
        "    # Item embeddings\n",
        "    for item_id, idx in mappings['item_to_idx'].items():\n",
        "        embedding_dict[f\"item_{item_id}\"] = item_embeddings[idx]\n",
        "    \n",
        "    # Save embeddings to S3 for later use\n",
        "    s3_client = boto_session.client('s3')\n",
        "    embeddings_s3_key = f\"fm-embeddings/user_embeddings.npy\"\n",
        "    items_embeddings_s3_key = f\"fm-embeddings/item_embeddings.npy\"\n",
        "    \n",
        "    # Save numpy arrays\n",
        "    user_buffer = io.BytesIO()\n",
        "    np.save(user_buffer, user_embeddings)\n",
        "    user_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(user_buffer, bucket, embeddings_s3_key)\n",
        "    \n",
        "    item_buffer = io.BytesIO()\n",
        "    np.save(item_buffer, item_embeddings)\n",
        "    item_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(item_buffer, bucket, items_embeddings_s3_key)\n",
        "    \n",
        "    print(f\"Embeddings saved to S3\")\n",
        "    \n",
        "    return user_embeddings, item_embeddings, embedding_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Top-100 K-Nearest Neighbors Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_knn_index(\n",
        "    item_embeddings: np.ndarray,\n",
        "    item_mappings: Dict,\n",
        "    n_neighbors: int = 100\n",
        ") -> Tuple[NearestNeighbors, Dict]:\n",
        "    \"\"\"\n",
        "    Build a K-NN index for item embeddings.\n",
        "    \n",
        "    Args:\n",
        "        item_embeddings: Item embedding matrix (num_items, embedding_dim)\n",
        "        item_mappings: Dictionary mapping item indices to parent_asin\n",
        "        n_neighbors: Number of neighbors to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (knn_model, idx_to_item_dict)\n",
        "    \"\"\"\n",
        "    # Build K-NN model\n",
        "    knn = NearestNeighbors(\n",
        "        n_neighbors=min(n_neighbors + 1, len(item_embeddings)),  # +1 to exclude self\n",
        "        metric='cosine',  # Use cosine similarity for embeddings\n",
        "        algorithm='brute'  # Brute force for exact results\n",
        "    )\n",
        "    \n",
        "    knn.fit(item_embeddings)\n",
        "    \n",
        "    # Create reverse mapping: embedding index -> parent_asin\n",
        "    idx_to_item = {idx: item for item, idx in item_mappings.items()}\n",
        "    \n",
        "    print(f\"K-NN index built with {len(item_embeddings)} items\")\n",
        "    \n",
        "    return knn, idx_to_item\n",
        "\n",
        "\n",
        "def retrieve_top_k_candidates(\n",
        "    user_id: str,\n",
        "    user_embeddings: np.ndarray,\n",
        "    item_embeddings: np.ndarray,\n",
        "    knn_model: NearestNeighbors,\n",
        "    user_mappings: Dict,\n",
        "    idx_to_item: Dict,\n",
        "    k: int = 100,\n",
        "    exclude_interacted: bool = True,\n",
        "    user_interactions: pd.DataFrame = None\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve top-K candidate items for a given user using FM embeddings.\n",
        "    \n",
        "    Strategy: Compute user-item similarity scores by taking dot product of\n",
        "    user embedding with all item embeddings, then select top-K.\n",
        "    \n",
        "    Args:\n",
        "        user_id: User ID to get recommendations for\n",
        "        user_embeddings: User embedding matrix\n",
        "        item_embeddings: Item embedding matrix\n",
        "        knn_model: Trained K-NN model (alternative method, not used in current implementation)\n",
        "        user_mappings: Dictionary mapping user_id to embedding index\n",
        "        idx_to_item: Dictionary mapping embedding index to parent_asin\n",
        "        k: Number of candidates to retrieve\n",
        "        exclude_interacted: Whether to exclude items user has already interacted with\n",
        "        user_interactions: DataFrame with user's past interactions\n",
        "    \n",
        "    Returns:\n",
        "        List of parent_asin values (top-K candidate items)\n",
        "    \"\"\"\n",
        "    # Get user embedding\n",
        "    if user_id not in user_mappings:\n",
        "        raise ValueError(f\"User {user_id} not found in embeddings\")\n",
        "    \n",
        "    user_idx = user_mappings[user_id]\n",
        "    user_embedding = user_embeddings[user_idx]  # Shape: (embedding_dim,)\n",
        "    \n",
        "    # Compute similarity scores: dot product of user embedding with all item embeddings\n",
        "    # This gives us a score for each item indicating how well it matches the user\n",
        "    similarity_scores = np.dot(item_embeddings, user_embedding)  # Shape: (num_items,)\n",
        "    \n",
        "    # Get top-K item indices (highest similarity scores)\n",
        "    top_k_indices = np.argsort(similarity_scores)[::-1][:k+1]  # +1 in case we need to exclude one\n",
        "    \n",
        "    # Convert indices to parent_asin\n",
        "    candidate_items = []\n",
        "    for idx in top_k_indices:\n",
        "        if idx in idx_to_item:\n",
        "            candidate_items.append(idx_to_item[idx])\n",
        "    \n",
        "    # Exclude items user has already interacted with\n",
        "    if exclude_interacted and user_interactions is not None:\n",
        "        user_items = set(user_interactions[\n",
        "            user_interactions['user_id'] == user_id\n",
        "        ]['parent_asin'].unique())\n",
        "        candidate_items = [item for item in candidate_items if item not in user_items]\n",
        "    \n",
        "    # Return top K (may be less if we excluded interactions)\n",
        "    return candidate_items[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ranking Stage: XGBoost with Feature Store Metadata\n",
        "\n",
        "### 3.1 Fetch Metadata from Online Feature Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_feature_store_metadata(\n",
        "    candidate_items: List[str],\n",
        "    user_id: str,\n",
        "    item_feature_group: FeatureGroup,\n",
        "    user_feature_group: FeatureGroup,\n",
        "    point_in_time: datetime = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch rich metadata from Online Feature Store for candidate items.\n",
        "    \n",
        "    This function performs point-in-time accurate joins to get:\n",
        "    - Item features: Price, Category, Average Rating, User Rating Count\n",
        "    - User features: User rating count, etc.\n",
        "    \n",
        "    Args:\n",
        "        candidate_items: List of parent_asin values\n",
        "        user_id: User ID for user features\n",
        "        item_feature_group: Item Feature Group object\n",
        "        user_feature_group: User Feature Group object\n",
        "        point_in_time: Point in time for feature retrieval (default: now)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with enriched features for each candidate item\n",
        "    \"\"\"\n",
        "    if point_in_time is None:\n",
        "        point_in_time = datetime.now()\n",
        "    \n",
        "    enriched_features = []\n",
        "    \n",
        "    # Fetch user features\n",
        "    user_features = None\n",
        "    try:\n",
        "        user_record = featurestore_runtime.get_record(\n",
        "            FeatureGroupName=user_feature_group.name,\n",
        "            RecordIdentifierValueAsString=user_id\n",
        "        )\n",
        "        if 'Record' in user_record:\n",
        "            user_features = {\n",
        "                feat['FeatureName']: feat['ValueAsString']\n",
        "                for feat in user_record['Record']\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not fetch user features for {user_id}: {e}\")\n",
        "    \n",
        "    # Fetch item features for each candidate\n",
        "    for parent_asin in candidate_items:\n",
        "        try:\n",
        "            item_record = featurestore_runtime.get_record(\n",
        "                FeatureGroupName=item_feature_group.name,\n",
        "                RecordIdentifierValueAsString=parent_asin\n",
        "            )\n",
        "            \n",
        "            if 'Record' in item_record:\n",
        "                item_features = {\n",
        "                    feat['FeatureName']: feat['ValueAsString']\n",
        "                    for feat in item_record['Record']\n",
        "                }\n",
        "                \n",
        "                # Combine user and item features\n",
        "                combined_features = {\n",
        "                    'parent_asin': parent_asin,\n",
        "                    'user_id': user_id\n",
        "                }\n",
        "                \n",
        "                # Add item features (handle different possible column names)\n",
        "                if 'price' in item_features:\n",
        "                    try:\n",
        "                        combined_features['item_price'] = float(item_features.get('price', 0))\n",
        "                    except:\n",
        "                        combined_features['item_price'] = 0.0\n",
        "                else:\n",
        "                    combined_features['item_price'] = 0.0\n",
        "                    \n",
        "                if 'main_category' in item_features:\n",
        "                    combined_features['item_category'] = item_features.get('main_category', '')\n",
        "                else:\n",
        "                    combined_features['item_category'] = ''\n",
        "                    \n",
        "                if 'average_rating' in item_features:\n",
        "                    try:\n",
        "                        combined_features['item_avg_rating'] = float(item_features.get('average_rating', 0))\n",
        "                    except:\n",
        "                        combined_features['item_avg_rating'] = 0.0\n",
        "                else:\n",
        "                    combined_features['item_avg_rating'] = 0.0\n",
        "                    \n",
        "                # Handle rating count (could be named differently)\n",
        "                rating_count = 0.0\n",
        "                for key in ['rating_count', 'user_rating_count', 'total_ratings']:\n",
        "                    if key in item_features:\n",
        "                        try:\n",
        "                            rating_count = float(item_features.get(key, 0))\n",
        "                            break\n",
        "                        except:\n",
        "                            pass\n",
        "                combined_features['item_rating_count'] = rating_count\n",
        "                \n",
        "                # Add user features\n",
        "                if user_features:\n",
        "                    if 'rating_count_by_user' in user_features:\n",
        "                        try:\n",
        "                            combined_features['user_rating_count'] = float(\n",
        "                                user_features.get('rating_count_by_user', 0)\n",
        "                            )\n",
        "                        except:\n",
        "                            combined_features['user_rating_count'] = 0.0\n",
        "                    else:\n",
        "                        combined_features['user_rating_count'] = 0.0\n",
        "                else:\n",
        "                    combined_features['user_rating_count'] = 0.0\n",
        "                \n",
        "                enriched_features.append(combined_features)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not fetch features for item {parent_asin}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if not enriched_features:\n",
        "        raise ValueError(\"No features retrieved from Feature Store\")\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    features_df = pd.DataFrame(enriched_features)\n",
        "    \n",
        "    # Fill missing values\n",
        "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
        "    features_df[numeric_cols] = features_df[numeric_cols].fillna(0)\n",
        "    \n",
        "    return features_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Prepare Training Data for XGBoost Ranker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_ranking_training_data(\n",
        "    interactions_df: pd.DataFrame,\n",
        "    user_feature_group: FeatureGroup,\n",
        "    item_feature_group: FeatureGroup,\n",
        "    sample_size: int = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare training data for XGBoost ranking model.\n",
        "    \n",
        "    For each user-item interaction, fetch features from Feature Store\n",
        "    at the point-in-time of the interaction.\n",
        "    \n",
        "    Args:\n",
        "        interactions_df: DataFrame with user_id, parent_asin, rating, event_time_seconds\n",
        "        user_feature_group: User Feature Group\n",
        "        item_feature_group: Item Feature Group\n",
        "        sample_size: Optional sample size for faster training (None = use all)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with features and target (rating)\n",
        "    \"\"\"\n",
        "    # Sample if needed\n",
        "    if sample_size and len(interactions_df) > sample_size:\n",
        "        interactions_df = interactions_df.sample(n=sample_size, random_state=42)\n",
        "    \n",
        "    print(f\"Preparing ranking training data for {len(interactions_df)} interactions...\")\n",
        "    \n",
        "    # Use Feature Store Dataset Builder for point-in-time joins\n",
        "    builder = feature_store.create_dataset(\n",
        "        base=interactions_df,\n",
        "        event_time_identifier_feature_name='event_time_seconds',\n",
        "        record_identifier_feature_name='user_id',\n",
        "        output_path=f\"s3://{bucket}/ranking-training-datasets/\"\n",
        "    )\n",
        "    \n",
        "    # Join with Feature Groups\n",
        "    builder = builder.with_feature_group(\n",
        "        feature_group=user_feature_group\n",
        "    )\n",
        "    \n",
        "    builder = builder.with_feature_group(\n",
        "        feature_group=item_feature_group\n",
        "    )\n",
        "    \n",
        "    # Generate the dataset\n",
        "    s3_uri, query = builder.to_csv_file()\n",
        "    \n",
        "    print(f\"Ranking training dataset created at: {s3_uri}\")\n",
        "    \n",
        "    # Load the dataset\n",
        "    ranking_df = pd.read_csv(s3_uri)\n",
        "    \n",
        "    # Clean column names (remove prefixes added by Feature Store)\n",
        "    ranking_df.columns = ranking_df.columns.str.replace(r'.*users.*\\.', '', regex=True)\n",
        "    ranking_df.columns = ranking_df.columns.str.replace(r'.*items.*\\.', '', regex=True)\n",
        "    \n",
        "    # Ensure rating is the target\n",
        "    if 'rating' not in ranking_df.columns:\n",
        "        raise ValueError(\"Rating column not found in training data\")\n",
        "    \n",
        "    print(f\"Training data shape: {ranking_df.shape}\")\n",
        "    print(f\"Features: {[col for col in ranking_df.columns if col != 'rating']}\")\n",
        "    \n",
        "    return ranking_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Train XGBoost Ranking Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_xgboost_ranker(\n",
        "    training_df: pd.DataFrame,\n",
        "    target_column: str = 'rating',\n",
        "    instance_type: str = 'ml.m5.xlarge'\n",
        ") -> Tuple[sagemaker.estimator.Estimator, List[str]]:\n",
        "    \"\"\"\n",
        "    Train an XGBoost model for ranking candidate items.\n",
        "    \n",
        "    Args:\n",
        "        training_df: DataFrame with features and target\n",
        "        target_column: Name of target column\n",
        "        instance_type: SageMaker instance type\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (trained XGBoost estimator, feature_names)\n",
        "    \"\"\"\n",
        "    # Separate features and target\n",
        "    feature_columns = [col for col in training_df.columns if col != target_column]\n",
        "    \n",
        "    # Remove ID columns from features\n",
        "    feature_columns = [col for col in feature_columns if col not in ['user_id', 'parent_asin', 'event_time_seconds', 'calendar_date']]\n",
        "    \n",
        "    X = training_df[feature_columns].copy()\n",
        "    y = training_df[target_column].copy()\n",
        "    \n",
        "    # Handle categorical features (one-hot encode)\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "    \n",
        "    # Fill missing values\n",
        "    X = X.fillna(0)\n",
        "    \n",
        "    # Convert to CSV format for XGBoost (label in first column)\n",
        "    training_data = pd.concat([y, X], axis=1)\n",
        "    \n",
        "    # Save to local file\n",
        "    local_file = '/tmp/xgboost_training.csv'\n",
        "    training_data.to_csv(local_file, index=False, header=False)\n",
        "    \n",
        "    # Upload to S3\n",
        "    s3_key = 'xgboost-ranking/training_data.csv'\n",
        "    s3_client = boto_session.client('s3')\n",
        "    s3_client.upload_file(local_file, bucket, s3_key)\n",
        "    \n",
        "    training_s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
        "    \n",
        "    # Save feature names for inference\n",
        "    feature_names_key = 'xgboost-ranking/feature_names.json'\n",
        "    feature_names_json = json.dumps(list(X.columns))\n",
        "    s3_client.put_object(\n",
        "        Bucket=bucket,\n",
        "        Key=feature_names_key,\n",
        "        Body=feature_names_json.encode('utf-8')\n",
        "    )\n",
        "    \n",
        "    # Get XGBoost container\n",
        "    container = image_uris.retrieve(\n",
        "        \"xgboost\",\n",
        "        boto_session.region_name,\n",
        "        version='1.7-1'\n",
        "    )\n",
        "    \n",
        "    # Create estimator\n",
        "    xgb_estimator = sagemaker.estimator.Estimator(\n",
        "        container,\n",
        "        role=role_arn,\n",
        "        instance_count=1,\n",
        "        instance_type=instance_type,\n",
        "        output_path=f\"s3://{bucket}/xgboost-model-artifacts/\",\n",
        "        sagemaker_session=sagemaker_session\n",
        "    )\n",
        "    \n",
        "    # Set hyperparameters\n",
        "    xgb_estimator.set_hyperparameters(\n",
        "        objective='reg:squarederror',  # Regression for rating prediction\n",
        "        num_round=100,\n",
        "        max_depth=6,\n",
        "        eta=0.3,\n",
        "        gamma=0,\n",
        "        min_child_weight=1,\n",
        "        subsample=0.8,\n",
        "        silent=0\n",
        "    )\n",
        "    \n",
        "    # Start training\n",
        "    print(\"Starting XGBoost training...\")\n",
        "    xgb_estimator.fit({'train': training_s3_uri})\n",
        "    \n",
        "    print(f\"XGBoost training complete! Model artifacts: {xgb_estimator.model_data}\")\n",
        "    \n",
        "    return xgb_estimator, list(X.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inference Pipeline: Complete Recommendation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoStageRecommender:\n",
        "    \"\"\"\n",
        "    Two-Stage Recommendation System:\n",
        "    1. Retrieval: FM embeddings + K-NN\n",
        "    2. Ranking: XGBoost with Feature Store metadata\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        user_embeddings: np.ndarray,\n",
        "        item_embeddings: np.ndarray,\n",
        "        knn_model: NearestNeighbors,\n",
        "        user_mappings: Dict,\n",
        "        idx_to_item: Dict,\n",
        "        xgb_predictor: sagemaker.predictor.Predictor,\n",
        "        feature_names: List[str],\n",
        "        user_feature_group: FeatureGroup,\n",
        "        item_feature_group: FeatureGroup,\n",
        "        user_interactions: pd.DataFrame = None\n",
        "    ):\n",
        "        self.user_embeddings = user_embeddings\n",
        "        self.item_embeddings = item_embeddings\n",
        "        self.knn_model = knn_model\n",
        "        self.user_mappings = user_mappings\n",
        "        self.idx_to_item = idx_to_item\n",
        "        self.xgb_predictor = xgb_predictor\n",
        "        self.feature_names = feature_names\n",
        "        self.user_feature_group = user_feature_group\n",
        "        self.item_feature_group = item_feature_group\n",
        "        self.user_interactions = user_interactions\n",
        "    \n",
        "    def get_recommendations(\n",
        "        self,\n",
        "        user_id: str,\n",
        "        top_k: int = 10,\n",
        "        retrieval_k: int = 100\n",
        "    ) -> List[Dict[str, any]]:\n",
        "        \"\"\"\n",
        "        Get top-K recommendations for a user.\n",
        "        \n",
        "        Pipeline:\n",
        "        1. Retrieve top-100 candidates using FM embeddings\n",
        "        2. Enrich candidates with Feature Store metadata\n",
        "        3. Rank candidates using XGBoost\n",
        "        4. Return top-K items\n",
        "        \n",
        "        Args:\n",
        "            user_id: User ID\n",
        "            top_k: Number of final recommendations to return\n",
        "            retrieval_k: Number of candidates to retrieve in stage 1\n",
        "        \n",
        "        Returns:\n",
        "            List of dictionaries with 'parent_asin' and 'predicted_rating'\n",
        "        \"\"\"\n",
        "        # Stage 1: Retrieval - Get top-K candidates\n",
        "        print(f\"Stage 1: Retrieving {retrieval_k} candidates for user {user_id}...\")\n",
        "        candidate_items = retrieve_top_k_candidates(\n",
        "            user_id=user_id,\n",
        "            user_embeddings=self.user_embeddings,\n",
        "            item_embeddings=self.item_embeddings,\n",
        "            knn_model=self.knn_model,\n",
        "            user_mappings=self.user_mappings,\n",
        "            idx_to_item=self.idx_to_item,\n",
        "            k=retrieval_k,\n",
        "            exclude_interacted=True,\n",
        "            user_interactions=self.user_interactions\n",
        "        )\n",
        "        \n",
        "        print(f\"Retrieved {len(candidate_items)} candidates\")\n",
        "        \n",
        "        if len(candidate_items) == 0:\n",
        "            return []\n",
        "        \n",
        "        # Stage 2: Enrich with Feature Store metadata\n",
        "        print(f\"Stage 2: Enriching candidates with Feature Store metadata...\")\n",
        "        enriched_features = fetch_feature_store_metadata(\n",
        "            candidate_items=candidate_items,\n",
        "            user_id=user_id,\n",
        "            item_feature_group=self.item_feature_group,\n",
        "            user_feature_group=self.user_feature_group\n",
        "        )\n",
        "        \n",
        "        print(f\"Enriched {len(enriched_features)} candidates\")\n",
        "        \n",
        "        # Stage 3: Rank with XGBoost\n",
        "        print(f\"Stage 3: Ranking candidates with XGBoost...\")\n",
        "        \n",
        "        # Prepare features for XGBoost (match training format)\n",
        "        feature_data = enriched_features.copy()\n",
        "        \n",
        "        # Handle categorical features (one-hot encode)\n",
        "        categorical_cols = feature_data.select_dtypes(include=['object']).columns\n",
        "        categorical_cols = [col for col in categorical_cols if col not in ['user_id', 'parent_asin']]\n",
        "        \n",
        "        if len(categorical_cols) > 0:\n",
        "            feature_data = pd.get_dummies(feature_data, columns=categorical_cols, drop_first=True)\n",
        "        \n",
        "        # Ensure all training features are present\n",
        "        for feat in self.feature_names:\n",
        "            if feat not in feature_data.columns:\n",
        "                feature_data[feat] = 0\n",
        "        \n",
        "        # Select only features used in training (in same order)\n",
        "        X = feature_data[self.feature_names]\n",
        "        \n",
        "        # Fill missing values\n",
        "        X = X.fillna(0)\n",
        "        \n",
        "        # Convert to CSV format for XGBoost (no header, no label column)\n",
        "        csv_data = X.to_csv(index=False, header=False)\n",
        "        \n",
        "        # Get predictions\n",
        "        predictions = self.xgb_predictor.predict(csv_data)\n",
        "        \n",
        "        # Parse predictions (XGBoost returns CSV string)\n",
        "        if isinstance(predictions, bytes):\n",
        "            predictions = predictions.decode('utf-8')\n",
        "        \n",
        "        pred_scores = [float(x) for x in predictions.strip().split('\\n') if x.strip()]\n",
        "        \n",
        "        # Combine with item IDs\n",
        "        results = [\n",
        "            {\n",
        "                'parent_asin': item,\n",
        "                'predicted_rating': score\n",
        "            }\n",
        "            for item, score in zip(candidate_items[:len(pred_scores)], pred_scores)\n",
        "        ]\n",
        "        \n",
        "        # Sort by predicted rating (descending)\n",
        "        results.sort(key=lambda x: x['predicted_rating'], reverse=True)\n",
        "        \n",
        "        # Return top-K\n",
        "        return results[:top_k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete Workflow: Training and Deployment\n",
        "\n",
        "Execute the following cells in order to train both models and set up the inference pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load interaction data\n",
        "interactions_df = pd.read_parquet(\n",
        "    \"s3://recommendation-project-rapid/processed/all_beauty_dataset/\",\n",
        "    engine=\"pyarrow\"\n",
        ")\n",
        "\n",
        "# Select only user_id, parent_asin, rating for FM training\n",
        "fm_training_df = interactions_df[['user_id', 'parent_asin', 'rating']].copy()\n",
        "\n",
        "print(f\"Loaded {len(fm_training_df)} interactions\")\n",
        "print(f\"Unique users: {fm_training_df['user_id'].nunique()}\")\n",
        "print(f\"Unique items: {fm_training_df['parent_asin'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Prepare and train Factorization Machines\n",
        "training_data_s3_uri, mappings, feature_dim = prepare_fm_training_data(\n",
        "    interactions_df=fm_training_df,\n",
        "    output_s3_path=f\"s3://{bucket}/fm-training/training_data.txt\"\n",
        ")\n",
        "\n",
        "# Train FM model\n",
        "fm_estimator = train_factorization_machines(\n",
        "    training_data_s3_uri=training_data_s3_uri,\n",
        "    feature_dim=feature_dim,\n",
        "    num_factors=64,\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Extract embeddings\n",
        "user_embeddings, item_embeddings, embedding_dict = extract_embeddings(\n",
        "    fm_estimator=fm_estimator,\n",
        "    mappings=mappings,\n",
        "    num_factors=64\n",
        ")\n",
        "\n",
        "# Build K-NN index\n",
        "knn_model, idx_to_item = build_knn_index(\n",
        "    item_embeddings=item_embeddings,\n",
        "    item_mappings=mappings['item_to_idx'],\n",
        "    n_neighbors=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Load Feature Groups\n",
        "# IMPORTANT: Update these names with your actual Feature Group names\n",
        "# You can find them by running: sagemaker_client.list_feature_groups()\n",
        "\n",
        "user_fg = FeatureGroup(\n",
        "    name=USER_FEATURE_GROUP_NAME,\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "item_fg = FeatureGroup(\n",
        "    name=ITEM_FEATURE_GROUP_NAME,\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Verify Feature Groups exist\n",
        "try:\n",
        "    user_fg.describe()\n",
        "    item_fg.describe()\n",
        "    print(\"Feature Groups loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Feature Groups: {e}\")\n",
        "    print(\"Please update USER_FEATURE_GROUP_NAME and ITEM_FEATURE_GROUP_NAME\")\n",
        "    print(\"\\nTo find your Feature Group names, run:\")\n",
        "    print(\"response = sagemaker_client.list_feature_groups()\")\n",
        "    print(\"for fg in response['FeatureGroupSummaries']:\")\n",
        "    print(\"    print(f\\\"- {fg['FeatureGroupName']}\\\")\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Prepare ranking training data\n",
        "ranking_training_df = prepare_ranking_training_data(\n",
        "    interactions_df=interactions_df,\n",
        "    user_feature_group=user_fg,\n",
        "    item_feature_group=item_fg,\n",
        "    sample_size=10000  # Use sample for faster training (remove for full dataset)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Train XGBoost ranker\n",
        "xgb_estimator, feature_names = train_xgboost_ranker(\n",
        "    training_df=ranking_training_df,\n",
        "    target_column='rating'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Deploy XGBoost model\n",
        "xgb_predictor = xgb_estimator.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type='ml.m5.large',\n",
        "    serializer=sagemaker.serializers.CSVSerializer(),\n",
        "    deserializer=sagemaker.deserializers.CSVDeserializer()\n",
        ")\n",
        "\n",
        "print(f\"XGBoost endpoint deployed: {xgb_predictor.endpoint_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Initialize Two-Stage Recommender\n",
        "recommender = TwoStageRecommender(\n",
        "    user_embeddings=user_embeddings,\n",
        "    item_embeddings=item_embeddings,\n",
        "    knn_model=knn_model,\n",
        "    user_mappings=mappings['user_to_idx'],\n",
        "    idx_to_item=idx_to_item,\n",
        "    xgb_predictor=xgb_predictor,\n",
        "    feature_names=feature_names,\n",
        "    user_feature_group=user_fg,\n",
        "    item_feature_group=item_fg,\n",
        "    user_interactions=interactions_df\n",
        ")\n",
        "\n",
        "print(\"Two-Stage Recommender initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Test the recommendation pipeline\n",
        "test_user_id = interactions_df['user_id'].iloc[0]\n",
        "\n",
        "recommendations = recommender.get_recommendations(\n",
        "    user_id=test_user_id,\n",
        "    top_k=10,\n",
        "    retrieval_k=100\n",
        ")\n",
        "\n",
        "print(f\"\\nTop 10 Recommendations for User: {test_user_id}\")\n",
        "print(\"=\" * 60)\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"{i}. Item: {rec['parent_asin']}, Predicted Rating: {rec['predicted_rating']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Standalone Inference Function\n",
        "\n",
        "For production use, here's a standalone function that can be called directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_recommendations(\n",
        "    user_id: str,\n",
        "    recommender: TwoStageRecommender,\n",
        "    top_k: int = 10\n",
        ") -> List[Dict[str, any]]:\n",
        "    \"\"\"\n",
        "    Standalone function to get recommendations for a user.\n",
        "    \n",
        "    This is the main entry point for the two-stage recommendation system.\n",
        "    It orchestrates:\n",
        "    (a) Retrieve 100 candidates via FM embeddings\n",
        "    (b) Enrich them with Feature Store metadata\n",
        "    (c) Rank them with XGBoost model\n",
        "    (d) Return the top 10 products\n",
        "    \n",
        "    Args:\n",
        "        user_id: User ID to get recommendations for\n",
        "        recommender: Initialized TwoStageRecommender instance\n",
        "        top_k: Number of recommendations to return (default: 10)\n",
        "    \n",
        "    Returns:\n",
        "        List of recommendation dictionaries with 'parent_asin' and 'predicted_rating'\n",
        "    \n",
        "    Example:\n",
        "        >>> recommendations = get_recommendations(\n",
        "        ...     user_id=\"AGKHLEW2SOWHNMFQI...\",\n",
        "        ...     recommender=recommender,\n",
        "        ...     top_k=10\n",
        "        ... )\n",
        "        >>> for rec in recommendations:\n",
        "        ...     print(f\"Item: {rec['parent_asin']}, Score: {rec['predicted_rating']:.2f}\")\n",
        "    \"\"\"\n",
        "    return recommender.get_recommendations(user_id=user_id, top_k=top_k)\n",
        "\n",
        "# Example usage:\n",
        "# recommendations = get_recommendations(\n",
        "#     user_id=\"AGKHLEW2SOWHNMFQI...\",\n",
        "#     recommender=recommender,\n",
        "#     top_k=10\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a complete Two-Stage Recommendation System:\n",
        "\n",
        "### Stage 1: Retrieval (Matrix Factorization)\n",
        "- **Model**: SageMaker Factorization Machines\n",
        "- **Input**: user_id and parent_asin interactions\n",
        "- **Output**: User and Item embeddings (64-dimensional vectors)\n",
        "- **Method**: K-Nearest Neighbors search to retrieve top-100 candidates\n",
        "\n",
        "### Stage 2: Ranking (XGBoost)\n",
        "- **Model**: SageMaker XGBoost\n",
        "- **Input**: Top-100 candidates enriched with Feature Store metadata\n",
        "  - Item features: Price, Category, Average Rating, Rating Count\n",
        "  - User features: User Rating Count\n",
        "- **Output**: Predicted ratings for each candidate\n",
        "- **Method**: Point-in-time accurate feature joins from Online Feature Store\n",
        "\n",
        "### Inference Pipeline\n",
        "The `get_recommendations()` function orchestrates the complete flow:\n",
        "1. Retrieves 100 candidates using FM embeddings\n",
        "2. Enriches candidates with real-time Feature Store metadata\n",
        "3. Ranks candidates using XGBoost\n",
        "4. Returns top-10 products\n",
        "\n",
        "### Key Features\n",
        "-  Point-in-time accurate feature joins\n",
        "-  Handles missing features gracefully\n",
        "-  Excludes already-interacted items\n",
        "-  Production-ready with proper error handling\n",
        "-  Uses SageMaker managed training and deployment\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
