{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Matrix Factorization Model (Retrieval Stage)\n",
        "\n",
        "This notebook trains the Matrix Factorization model for the retrieval stage of the two-stage recommendation system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import io\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, List\n",
        "from datetime import datetime\n",
        "\n",
        "# Install surprise library if not already installed\n",
        "try:\n",
        "    import surprise\n",
        "except ImportError:\n",
        "    print(\"Installing surprise library...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"scikit-surprise\"])\n",
        "    import surprise\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from surprise import SVD, Dataset, Reader\n",
        "\n",
        "# Configuration\n",
        "def get_config():\n",
        "    \"\"\"Get configuration dictionary.\"\"\"\n",
        "    return {\n",
        "        'region': \"ap-south-1\",\n",
        "        'role_arn': \"arn:aws:iam::487512486150:role/recommendationsystem-sagemaker-role\",\n",
        "        'bucket': \"amazon-sagemaker-local-dev-store\",\n",
        "        'user_feature_group_name': \"all-beauty-users-1766218384\",\n",
        "        'item_feature_group_name': \"all-beauty-items-1766218384\"\n",
        "    }\n",
        "\n",
        "def initialize_sessions(config: dict = None):\n",
        "    \"\"\"Initialize AWS and SageMaker sessions.\"\"\"\n",
        "    if config is None:\n",
        "        config = get_config()\n",
        "    \n",
        "    boto_session = boto3.Session(region_name=config['region'])\n",
        "    sagemaker_session = sagemaker.Session(\n",
        "        boto_session=boto_session,\n",
        "        default_bucket=config['bucket']\n",
        "    )\n",
        "    featurestore_runtime = boto_session.client(\n",
        "        service_name='sagemaker-featurestore-runtime',\n",
        "        region_name=config['region']\n",
        "    )\n",
        "    sagemaker_client = boto_session.client(\n",
        "        service_name='sagemaker',\n",
        "        region_name=config['region']\n",
        "    )\n",
        "    \n",
        "    print(f\"Initialized SageMaker session in {config['region']}\")\n",
        "    print(f\"Default bucket: {config['bucket']}\")\n",
        "    \n",
        "    return {\n",
        "        'boto_session': boto_session,\n",
        "        'sagemaker_session': sagemaker_session,\n",
        "        'featurestore_runtime': featurestore_runtime,\n",
        "        'sagemaker_client': sagemaker_client,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "# Matrix Factorization Functions\n",
        "def prepare_mf_training_data(interactions_df, output_s3_path, bucket, boto_session):\n",
        "    \"\"\"Prepare training data for Matrix Factorization using Surprise library.\"\"\"\n",
        "    unique_users = sorted(interactions_df['user_id'].unique())\n",
        "    unique_items = sorted(interactions_df['parent_asin'].unique())\n",
        "    \n",
        "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
        "    \n",
        "    num_users = len(unique_users)\n",
        "    num_items = len(unique_items)\n",
        "    \n",
        "    print(f\"Number of unique users: {num_users}\")\n",
        "    print(f\"Number of unique items: {num_items}\")\n",
        "    \n",
        "    training_data = interactions_df[['user_id', 'parent_asin', 'rating']].copy()\n",
        "    local_file = '/tmp/mf_training_data.csv'\n",
        "    training_data.to_csv(local_file, index=False, header=False)\n",
        "    \n",
        "    s3_client = boto_session.client('s3')\n",
        "    s3_path = output_s3_path.replace(f's3://{bucket}/', '')\n",
        "    s3_client.upload_file(local_file, bucket, s3_path)\n",
        "    \n",
        "    s3_uri = f\"s3://{bucket}/{s3_path}\"\n",
        "    print(f\"Training data saved to: {s3_uri}\")\n",
        "    \n",
        "    mappings = {\n",
        "        'user_to_idx': user_to_idx,\n",
        "        'item_to_idx': item_to_idx,\n",
        "        'idx_to_user': {idx: user for user, idx in user_to_idx.items()},\n",
        "        'idx_to_item': {idx: item for item, idx in item_to_idx.items()},\n",
        "        'num_users': num_users,\n",
        "        'num_items': num_items\n",
        "    }\n",
        "    \n",
        "    mappings_key = s3_path.replace('.csv', '_mappings.pkl')\n",
        "    mappings_buffer = io.BytesIO()\n",
        "    pickle.dump(mappings, mappings_buffer)\n",
        "    mappings_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(mappings_buffer, bucket, mappings_key)\n",
        "    \n",
        "    return s3_uri, mappings\n",
        "\n",
        "def train_matrix_factorization(training_data_s3_uri, mappings, bucket, sagemaker_session, boto_session, \n",
        "                               n_factors=64, n_epochs=20, lr_all=0.005, reg_all=0.02):\n",
        "    \"\"\"Train a Matrix Factorization model using Surprise library's SVD algorithm.\"\"\"\n",
        "    s3_client = boto_session.client('s3')\n",
        "    s3_path = training_data_s3_uri.replace('s3://', '').replace(f'{bucket}/', '')\n",
        "    local_file = '/tmp/mf_training_data.csv'\n",
        "    \n",
        "    print(f\"Downloading training data from {training_data_s3_uri}...\")\n",
        "    s3_client.download_file(bucket, s3_path, local_file)\n",
        "    \n",
        "    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))\n",
        "    data = Dataset.load_from_file(local_file, reader=reader)\n",
        "    trainset = data.build_full_trainset()\n",
        "    \n",
        "    model = SVD(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all, random_state=42)\n",
        "    \n",
        "    print(\"Starting Matrix Factorization training with Surprise SVD...\")\n",
        "    model.fit(trainset)\n",
        "    print(\"Training complete!\")\n",
        "    \n",
        "    num_users = mappings['num_users']\n",
        "    num_items = mappings['num_items']\n",
        "    \n",
        "    user_embeddings = np.zeros((num_users, n_factors))\n",
        "    item_embeddings = np.zeros((num_items, n_factors))\n",
        "    \n",
        "    for inner_uid in range(trainset.n_users):\n",
        "        try:\n",
        "            raw_uid = trainset.to_raw_uid(inner_uid)\n",
        "            if raw_uid in mappings['user_to_idx']:\n",
        "                user_idx = mappings['user_to_idx'][raw_uid]\n",
        "                user_embeddings[user_idx] = model.pu[inner_uid]\n",
        "        except (KeyError, IndexError):\n",
        "            continue\n",
        "    \n",
        "    for inner_iid in range(trainset.n_items):\n",
        "        try:\n",
        "            raw_iid = trainset.to_raw_iid(inner_iid)\n",
        "            if raw_iid in mappings['item_to_idx']:\n",
        "                item_idx = mappings['item_to_idx'][raw_iid]\n",
        "                item_embeddings[item_idx] = model.qi[inner_iid]\n",
        "        except (KeyError, IndexError):\n",
        "            continue\n",
        "    \n",
        "    print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
        "    print(f\"Item embeddings shape: {item_embeddings.shape}\")\n",
        "    \n",
        "    embeddings_s3_key = f\"mf-embeddings/user_embeddings.npy\"\n",
        "    items_embeddings_s3_key = f\"mf-embeddings/item_embeddings.npy\"\n",
        "    \n",
        "    user_buffer = io.BytesIO()\n",
        "    np.save(user_buffer, user_embeddings)\n",
        "    user_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(user_buffer, bucket, embeddings_s3_key)\n",
        "    \n",
        "    item_buffer = io.BytesIO()\n",
        "    np.save(item_buffer, item_embeddings)\n",
        "    item_buffer.seek(0)\n",
        "    s3_client.upload_fileobj(item_buffer, bucket, items_embeddings_s3_key)\n",
        "    \n",
        "    print(f\"Embeddings saved to S3\")\n",
        "    return user_embeddings, item_embeddings\n",
        "\n",
        "def build_knn_index(item_embeddings, item_mappings, n_neighbors=100):\n",
        "    \"\"\"Build a K-NN index for item embeddings.\"\"\"\n",
        "    knn = NearestNeighbors(\n",
        "        n_neighbors=min(n_neighbors + 1, len(item_embeddings)),\n",
        "        metric='cosine',\n",
        "        algorithm='brute'\n",
        "    )\n",
        "    knn.fit(item_embeddings)\n",
        "    idx_to_item = {idx: item for item, idx in item_mappings.items()}\n",
        "    print(f\"K-NN index built with {len(item_embeddings)} items\")\n",
        "    return knn, idx_to_item\n",
        "\n",
        "def retrieve_top_k_candidates(user_id, user_embeddings, item_embeddings, user_mappings, idx_to_item, \n",
        "                              k=100, exclude_interacted=True, user_interactions=None):\n",
        "    \"\"\"Retrieve top-K candidate items for a given user using MF embeddings.\"\"\"\n",
        "    if user_id not in user_mappings:\n",
        "        raise ValueError(f\"User {user_id} not found in embeddings\")\n",
        "    \n",
        "    user_idx = user_mappings[user_id]\n",
        "    user_embedding = user_embeddings[user_idx]\n",
        "    similarity_scores = np.dot(item_embeddings, user_embedding)\n",
        "    top_k_indices = np.argsort(similarity_scores)[::-1][:k+1]\n",
        "    \n",
        "    candidate_items = []\n",
        "    for idx in top_k_indices:\n",
        "        if idx in idx_to_item:\n",
        "            candidate_items.append(idx_to_item[idx])\n",
        "    \n",
        "    if exclude_interacted and user_interactions is not None:\n",
        "        user_items = set(user_interactions[user_interactions['user_id'] == user_id]['parent_asin'].unique())\n",
        "        candidate_items = [item for item in candidate_items if item not in user_items]\n",
        "    \n",
        "    return candidate_items[:k]\n",
        "\n",
        "# Model Registry and DynamoDB Functions\n",
        "def save_mf_model_to_registry(model_artifacts_s3_uri, model_name, model_package_group_name, \n",
        "                              role_arn, sagemaker_session, boto_session, model_description):\n",
        "    \"\"\"Save Matrix Factorization model to SageMaker Model Registry.\"\"\"\n",
        "    sagemaker_client = boto_session.client('sagemaker')\n",
        "    \n",
        "    try:\n",
        "        sagemaker_client.describe_model_package_group(ModelPackageGroupName=model_package_group_name)\n",
        "    except sagemaker_client.exceptions.ResourceNotFound:\n",
        "        sagemaker_client.create_model_package_group(\n",
        "            ModelPackageGroupName=model_package_group_name,\n",
        "            ModelPackageGroupDescription=\"Matrix Factorization retrieval models\"\n",
        "        )\n",
        "        print(f\"Created model package group: {model_package_group_name}\")\n",
        "    \n",
        "    try:\n",
        "        model_package_input_dict = {\n",
        "            \"ModelPackageGroupName\": model_package_group_name,\n",
        "            \"ModelPackageDescription\": model_description,\n",
        "            \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
        "            \"MetadataProperties\": {\n",
        "                \"GeneratedBy\": \"SageMaker Pipeline\",\n",
        "                \"ProjectId\": \"two-stage-recommendation-system\"\n",
        "            },\n",
        "            \"CustomerMetadataProperties\": {\n",
        "                \"ModelArtifactsS3Uri\": model_artifacts_s3_uri,\n",
        "                \"ModelType\": \"MatrixFactorization\",\n",
        "                \"TrainingFramework\": \"Surprise\"\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        model_package_response = sagemaker_client.create_model_package(**model_package_input_dict)\n",
        "        model_package_arn = model_package_response['ModelPackageArn']\n",
        "        print(f\"Model package created: {model_package_arn}\")\n",
        "        return model_package_arn\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating model package: {e}\")\n",
        "        return f\"arn:aws:sagemaker:{boto_session.region_name}:model-package/{model_package_group_name}/manual\"\n",
        "\n",
        "def store_candidates_in_dynamodb(user_embeddings, item_embeddings, user_mappings, idx_to_item,\n",
        "                                 interactions_df, dynamodb_table_name, boto_session, k=100, batch_size=25):\n",
        "    \"\"\"Generate candidates for all users and store them in DynamoDB.\"\"\"\n",
        "    dynamodb = boto_session.resource('dynamodb')\n",
        "    table = dynamodb.Table(dynamodb_table_name)\n",
        "    \n",
        "    all_user_ids = list(user_mappings.keys())\n",
        "    num_users = len(all_user_ids)\n",
        "    \n",
        "    print(f\"Generating candidates for {num_users} users...\")\n",
        "    print(f\"Storing in DynamoDB table: {dynamodb_table_name}\")\n",
        "    \n",
        "    for batch_start in range(0, num_users, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, num_users)\n",
        "        batch_users = all_user_ids[batch_start:batch_end]\n",
        "        \n",
        "        with table.batch_writer() as batch:\n",
        "            for user_id in batch_users:\n",
        "                try:\n",
        "                    candidate_items = retrieve_top_k_candidates(\n",
        "                        user_id=user_id,\n",
        "                        user_embeddings=user_embeddings,\n",
        "                        item_embeddings=item_embeddings,\n",
        "                        user_mappings=user_mappings,\n",
        "                        idx_to_item=idx_to_item,\n",
        "                        k=k,\n",
        "                        exclude_interacted=True,\n",
        "                        user_interactions=interactions_df\n",
        "                    )\n",
        "                    \n",
        "                    item = {\n",
        "                        'user_id': user_id,\n",
        "                        'candidates': candidate_items,\n",
        "                        'num_candidates': len(candidate_items),\n",
        "                        'timestamp': datetime.utcnow().isoformat()\n",
        "                    }\n",
        "                    batch.put_item(Item=item)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing user {user_id}: {e}\")\n",
        "                    continue\n",
        "        \n",
        "        if batch_end % 100 == 0 or batch_end == num_users:\n",
        "            print(f\"Processed {batch_end}/{num_users} users ({100*batch_end/num_users:.1f}%)...\")\n",
        "    \n",
        "    print(f\"\\nCompleted! Stored candidates for {num_users} users in DynamoDB table: {dynamodb_table_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize sessions\n",
        "config = get_config()\n",
        "sessions = initialize_sessions(config)\n",
        "\n",
        "boto_session = sessions['boto_session']\n",
        "sagemaker_session = sessions['sagemaker_session']\n",
        "config = sessions['config']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Interaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load interaction data\n",
        "interactions_df = pd.read_parquet(\n",
        "    \"s3://recommendation-project-rapid/processed/all_beauty_dataset/\",\n",
        "    engine=\"pyarrow\"\n",
        ")\n",
        "\n",
        "# Select only user_id, parent_asin, rating for MF training\n",
        "mf_training_df = interactions_df[['user_id', 'parent_asin', 'rating']].copy()\n",
        "\n",
        "print(f\"Loaded {len(mf_training_df)} interactions\")\n",
        "print(f\"Unique users: {mf_training_df['user_id'].nunique()}\")\n",
        "print(f\"Unique items: {mf_training_df['parent_asin'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare MF training data\n",
        "training_data_s3_uri, mappings = prepare_mf_training_data(\n",
        "    interactions_df=mf_training_df,\n",
        "    output_s3_path=f\"s3://{config['bucket']}/mf-training/training_data.csv\",\n",
        "    bucket=config['bucket'],\n",
        "    boto_session=boto_session\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Matrix Factorization Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Matrix Factorization model using Surprise\n",
        "user_embeddings, item_embeddings = train_matrix_factorization(\n",
        "    training_data_s3_uri=training_data_s3_uri,\n",
        "    mappings=mappings,\n",
        "    bucket=config['bucket'],\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    boto_session=boto_session,\n",
        "    n_factors=64,      # Number of latent factors (embedding dimension)\n",
        "    n_epochs=20,       # Number of training epochs\n",
        "    lr_all=0.005,      # Learning rate\n",
        "    reg_all=0.02       # Regularization term\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embeddings Ready\n",
        "\n",
        "Embeddings are already extracted from the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embeddings are already available from training\n",
        "print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
        "print(f\"Item embeddings shape: {item_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build K-NN Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build K-NN index for item embeddings\n",
        "knn_model, idx_to_item = build_knn_index(\n",
        "    item_embeddings=item_embeddings,\n",
        "    item_mappings=mappings['item_to_idx'],\n",
        "    n_neighbors=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Artifacts\n",
        "\n",
        "Save the trained model artifacts for use in inference:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model artifacts to S3 for Model Registry\n",
        "s3_client = boto_session.client('s3')\n",
        "mf_artifacts = {\n",
        "    'mappings': mappings,\n",
        "    'user_embeddings': user_embeddings,\n",
        "    'item_embeddings': item_embeddings,\n",
        "    'knn_model': knn_model,\n",
        "    'idx_to_item': idx_to_item\n",
        "}\n",
        "\n",
        "# Save to S3\n",
        "model_artifacts_key = f\"mf-model-artifacts/mf_model_artifacts.pkl\"\n",
        "artifacts_buffer = io.BytesIO()\n",
        "pickle.dump(mf_artifacts, artifacts_buffer)\n",
        "artifacts_buffer.seek(0)\n",
        "s3_client.upload_fileobj(artifacts_buffer, config['bucket'], model_artifacts_key)\n",
        "\n",
        "model_artifacts_s3_uri = f\"s3://{config['bucket']}/{model_artifacts_key}\"\n",
        "print(f\"Model artifacts saved to S3: {model_artifacts_s3_uri}\")\n",
        "print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
        "print(f\"Item embeddings shape: {item_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Model to SageMaker Model Registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model to SageMaker Model Registry\n",
        "model_name = \"mf-retrieval-model\"\n",
        "model_package_group_name = \"mf-retrieval-models\"\n",
        "\n",
        "model_package_arn = save_mf_model_to_registry(\n",
        "    model_artifacts_s3_uri=model_artifacts_s3_uri,\n",
        "    model_name=model_name,\n",
        "    model_package_group_name=model_package_group_name,\n",
        "    role_arn=config['role_arn'],\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    boto_session=boto_session,\n",
        "    model_description=\"Matrix Factorization model for candidate retrieval in two-stage recommendation system\"\n",
        ")\n",
        "\n",
        "print(f\"Model registered: {model_package_arn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Generate and Store Candidates in DynamoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load full interactions for excluding already-interacted items\n",
        "full_interactions_df = pd.read_parquet(\n",
        "    \"s3://recommendation-project-rapid/processed/all_beauty_dataset/\",\n",
        "    engine=\"pyarrow\"\n",
        ")\n",
        "\n",
        "# DynamoDB table name (create this table first if it doesn't exist)\n",
        "dynamodb_table_name = \"user-candidates\"  # Update with your table name\n",
        "\n",
        "# Generate and store candidates in DynamoDB\n",
        "store_candidates_in_dynamodb(\n",
        "    user_embeddings=user_embeddings,\n",
        "    item_embeddings=item_embeddings,\n",
        "    user_mappings=mappings['user_to_idx'],\n",
        "    idx_to_item=idx_to_item,\n",
        "    interactions_df=full_interactions_df,\n",
        "    dynamodb_table_name=dynamodb_table_name,\n",
        "    boto_session=boto_session,\n",
        "    k=100,  # Top 100 candidates per user\n",
        "    batch_size=25  # DynamoDB batch write limit\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
