{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWS Glue: Feature Store Data Processing\n",
        "\n",
        "This notebook processes data for Feature Stores used in the XGBoost ranking model.\n",
        "It runs in AWS Glue and prepares User and Item Feature Groups.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Glue Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "# Initialize Glue Context\n",
        "sc = SparkContext.getOrCreate()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, count, avg, countDistinct\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Read and Process Reviews Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# S3 paths\n",
        "reviews_input = \"s3://recommendation-project-rapid/raw/All_Beauty.jsonl\"\n",
        "metadata_input = \"s3://recommendation-project-rapid/raw/meta_All_Beauty.jsonl\"\n",
        "\n",
        "# Load Reviews\n",
        "df_reviews = spark.read.json(reviews_input)\n",
        "\n",
        "# Process reviews with event time\n",
        "reviews_cleaned = (\n",
        "    df_reviews.select(\n",
        "        col(\"user_id\"),\n",
        "        col(\"parent_asin\"),\n",
        "        col(\"rating\").cast(\"double\"),\n",
        "        col(\"timestamp\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"event_time_seconds\", (col(\"timestamp\") / 1000).cast(\"double\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"calendar_date\", from_unixtime(col(\"event_time_seconds\"), \"yyyy-MM-dd\")\n",
        "    )\n",
        "    .drop(\"timestamp\")\n",
        ")\n",
        "\n",
        "print(f\"Loaded {df_reviews.count()} reviews\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Read and Process Metadata Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Metadata\n",
        "df_metadata = spark.read.json(metadata_input)\n",
        "\n",
        "# Select columns needed for Feature Store\n",
        "metadata_cleaned = df_metadata.select(\n",
        "    col(\"parent_asin\").alias(\"meta_parent_asin\"),\n",
        "    col(\"title\").alias(\"movie_title\"),\n",
        "    col(\"main_category\"),\n",
        "    col(\"price\").cast(\"double\").alias(\"price\")  # Add price if available\n",
        ")\n",
        "\n",
        "print(f\"Loaded {df_metadata.count()} metadata records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create User Feature Group Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate user features\n",
        "user_features = (\n",
        "    reviews_cleaned\n",
        "    .groupBy(\"user_id\", \"event_time_seconds\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"rating_count_by_user\"),\n",
        "        avg(\"rating\").alias(\"avg_rating_by_user\")\n",
        "    )\n",
        "    .select(\n",
        "        col(\"user_id\"),\n",
        "        col(\"event_time_seconds\"),\n",
        "        col(\"rating_count_by_user\").cast(\"double\"),\n",
        "        col(\"avg_rating_by_user\").cast(\"double\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"User features shape: {user_features.count()} records\")\n",
        "user_features.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Item Feature Group Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join reviews with metadata\n",
        "reviews_with_metadata = reviews_cleaned.join(\n",
        "    metadata_cleaned,\n",
        "    reviews_cleaned.parent_asin == metadata_cleaned.meta_parent_asin,\n",
        "    \"inner\"\n",
        ").drop(\"meta_parent_asin\")\n",
        "\n",
        "# Aggregate item features\n",
        "item_features = (\n",
        "    reviews_with_metadata\n",
        "    .groupBy(\"parent_asin\", \"event_time_seconds\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"rating_count\"),\n",
        "        avg(\"rating\").alias(\"average_rating\"),\n",
        "        f.first(\"main_category\").alias(\"main_category\"),\n",
        "        f.first(\"price\").alias(\"price\")\n",
        "    )\n",
        "    .select(\n",
        "        col(\"parent_asin\"),\n",
        "        col(\"event_time_seconds\"),\n",
        "        col(\"rating_count\").cast(\"double\"),\n",
        "        col(\"average_rating\").cast(\"double\"),\n",
        "        col(\"main_category\"),\n",
        "        col(\"price\").cast(\"double\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Item features shape: {item_features.count()} records\")\n",
        "item_features.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save to S3 for Feature Store Ingestion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output paths\n",
        "user_features_output = \"s3://recommendation-project-rapid/feature-store/users/\"\n",
        "item_features_output = \"s3://recommendation-project-rapid/feature-store/items/\"\n",
        "\n",
        "# Write user features\n",
        "user_features.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"event_time_seconds\") \\\n",
        "    .parquet(user_features_output)\n",
        "\n",
        "# Write item features\n",
        "item_features.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"event_time_seconds\") \\\n",
        "    .parquet(item_features_output)\n",
        "\n",
        "print(f\"User features saved to: {user_features_output}\")\n",
        "print(f\"Item features saved to: {item_features_output}\")\n",
        "print(\"Job Complete! Data ready for Feature Store ingestion.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
